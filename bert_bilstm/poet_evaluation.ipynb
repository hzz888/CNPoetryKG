{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdOoXiWfngIz"
   },
   "outputs": [],
   "source": [
    "! pip install -qq transformers\n",
    "! pip install -qq --upgrade xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezHE_FBvKLfb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from _thread import start_new_thread\n",
    "from functools import wraps\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5120,
     "status": "ok",
     "timestamp": 1650957586870,
     "user": {
      "displayName": "tianqianjin lin",
      "userId": "02429681390610284112"
     },
     "user_tz": -480
    },
    "id": "DOUl9_XlnNLy",
    "outputId": "fdd193da-4a39-413b-e63d-72aa5f54d618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "root = 'drive/MyDrive/Colab Notebooks/BertLSTMCLF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "830bptL8pToK"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEFO-KqTnRi2"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, n_layers, n_classes):\n",
    "        super().__init__()\n",
    "        # BERT layers\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-chinese\") # pretrained_model_name_or_path = \".\"\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = self.bert.config.hidden_size, \n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = n_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 * n_layers, hidden_dim), # Linear layer\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoded_input):\n",
    "        output = self.bert(**encoded_input)\n",
    "        # print(output.last_hidden_state.shape)\n",
    "        embedded = output.last_hidden_state\n",
    "\n",
    "        self.lstm.flatten_parameters() # for warning below\n",
    "        # UserWarning: RNN module weights are not part of single contiguous chunk of memory. \n",
    "        # This means they need to be compacted at every call, possibly greatly increasing memory usage. \n",
    "        # To compact weights again call flatten_parameters(). \n",
    "        # text -> [batch_size, sequence_length]\n",
    "        # embedded = self.embedding(text) # Create embedding of the input text  text -> [batch_size, sequence_length, emb_dim]\n",
    "        # Handle padding to ignore padding during training of the RNN\n",
    "        \n",
    "        # disable the pack_padded_sequence function for the ability to handle empty token list\n",
    "        # packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.lstm(embedded) # hidden -> [num_direction*num_layers, batch_size, emb_dim]\n",
    "     \n",
    "        # hidden -> [batch_size, emb_dim*num_direction*num_layers] \n",
    "        hidden = hidden.permute(1,0,2).reshape(output.shape[0], -1) # Concatenate the forward and backward hidden state of each layer            \n",
    "\n",
    "        output = self.out(hidden) # [batch_size, n_classes]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AK62GPiQn6M_"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(f\"{root}/final.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1944941,
     "status": "ok",
     "timestamp": 1650959531807,
     "user": {
      "displayName": "tianqianjin lin",
      "userId": "02429681390610284112"
     },
     "user_tz": -480
    },
    "id": "iq2c0pClp6Pt",
    "outputId": "f7c5dfdd-7a79-435c-b5f7-c63925853f07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "6798it [32:14,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# create model and move it to GPU with id rank\n",
    "moods = {0: '喜悦', 1: '愤怒', 2: '厌恶', 3: '低落'}\n",
    "\n",
    "model = SentimentClassifier(hidden_dim=256, n_layers=2, n_classes=4)\n",
    "model.load_state_dict(torch.load(f'{root}/3_0.6505.pt'))\n",
    "model = model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = \"bert-base-chinese\") # bert-base-uncased\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    eval_moods = []\n",
    "    for row_idx, row_content in tqdm(data.iterrows()):\n",
    "        text = list(filter(\n",
    "            lambda x: len(x)>5, \n",
    "            (row_content['translation'] \\\n",
    "            + '\\n' + row_content['appreciation'] \\\n",
    "            + '\\n' + row_content['background']).split('\\n')\n",
    "        ))\n",
    "        if len(text)> 1:\n",
    "            # print(text)\n",
    "            encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "            encoded_input = {k:v.to(device) for k,v in encoded_input.items()}\n",
    "            output = model(encoded_input).softmax(dim=1).mean(0).cpu().numpy()\n",
    "            eval_moods.append(output)\n",
    "            # print(output)\n",
    "            # break\n",
    "        else:\n",
    "            eval_moods.append(np.zeros(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1650959531808,
     "user": {
      "displayName": "tianqianjin lin",
      "userId": "02429681390610284112"
     },
     "user_tz": -480
    },
    "id": "mk3-ZV3ztvWb",
    "outputId": "ffa68f6f-7257-4c9f-81a3-41180d758ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我好难过啊，今天好倒霉']\n",
      "tensor([0.0050, 0.0343, 0.4493, 0.5114], device='cuda:0')\n",
      "['我好开心啊！！！今天中奖了！']\n",
      "tensor([0.8622, 0.1034, 0.0173, 0.0170], device='cuda:0')\n",
      "['想不通怎么可以这样呢？这是人做的事吗？']\n",
      "tensor([0.1087, 0.8396, 0.0273, 0.0245], device='cuda:0')\n",
      "['这也太恶心了']\n",
      "tensor([0.0864, 0.5080, 0.2079, 0.1977], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "text=['我好难过啊，今天好倒霉']\n",
    "print(text)\n",
    "with torch.no_grad():\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "    encoded_input = {k:v.to(device) for k,v in encoded_input.items()}\n",
    "    output = model(encoded_input).softmax(dim=1).mean(0)\n",
    "    print(output)\n",
    "\n",
    "text=['我好开心啊！！！今天中奖了！']\n",
    "print(text)\n",
    "with torch.no_grad():\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "    encoded_input = {k:v.to(device) for k,v in encoded_input.items()}\n",
    "    output = model(encoded_input).softmax(dim=1).mean(0)\n",
    "    print(output)\n",
    "\n",
    "text=['想不通怎么可以这样呢？这是人做的事吗？']\n",
    "print(text)\n",
    "with torch.no_grad():\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "    encoded_input = {k:v.to(device) for k,v in encoded_input.items()}\n",
    "    output = model(encoded_input).softmax(dim=1).mean(0)\n",
    "    print(output)\n",
    "\n",
    "text=['这也太恶心了']\n",
    "print(text)\n",
    "with torch.no_grad():\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "    encoded_input = {k:v.to(device) for k,v in encoded_input.items()}\n",
    "    output = model(encoded_input).softmax(dim=1).mean(0)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZ-7hmlktWet"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=eval_moods, columns = list(moods.values())).to_csv(f'{root}/BertBiLSTM_Eval_Moods.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPgFK47aOvE6lts61it1CtN",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "poet_evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
